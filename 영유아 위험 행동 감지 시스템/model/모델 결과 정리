- 2 layer(32, 64)
0.001
0.0005
0.0003
0.0001

- 2 layer * 2(32, 64)
0.001
0.0005
0.0003
0.0001 : 58%에 epoch 20까지(dense 128)/ 54%에 epoch 20까지(dense 256)

- 2 layer * 2(64, 32)
0.001
0.0005
0.0003
0.0001 : 25%, 학슴 안됨(망함)

- 3 layer (32, 64 ,128)
0.001
0.0005
0.0003
0.0001 : 60%에 epoch 20까지

- 3 layer (128, 64, 32)
0.001 : 50~45%, 학습 진행 잘 안됨(dropout 0.2)
0.0005 : 61%에 epoch 35까지(dropout 0.5) / 62%에 epoch 35까지(dropout 0.2)
0.0003 : 45%에 epoch 40까지(dense 256, 128) 비교적 학습 잘됨
0.0001 : 53%에 epoch 35까지(dense 128)

- 3 layer * 2(32, 64, 128)
0.001
0.0005 : 학습 제대로 안됨 - 다시 돌리는중
0.0003
0.0001 : 55%에 epoch 35까지(dense 128)/ 60%에 epoch 20까지(dense 256)

- 3 layer * 2(128. 64, 32)
0.001
0.0005
0.0003
0.0001

- 4 layer (256, 128, 64, 32)
0.001 : 25%(학습 잘 안됨)
0.0005
0.0003
0.0001

*convLSTM2D activation function에 relu 적용 -> 오히려 정확도 떨어지고 학습 안됨
